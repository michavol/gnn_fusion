{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to evaluate molecule graph regression models (individual, fused or ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    IMPORTING LIBS\n",
    "\"\"\"\n",
    "import dgl\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import argparse, json\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ehoskovec/Workspace/ETH/master/sem3/deepLearning/Project/gnn_fusion/gnn_benchmarking\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# path = os.path.abspath(\"eval_molecules_graph_regression.ipynb\")\n",
    "# os.chdir(path) # go to root folder of the project\n",
    "# os.chdir(\"../\")\n",
    "os.chdir(\"gnn_benchmarking\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    IMPORTING CUSTOM MODULES/METHODS\n",
    "\"\"\"\n",
    "from nets.molecules_graph_regression.load_net import gnn_model # import all GNNS\n",
    "from data.data import LoadData # import dataset\n",
    "import train.train_molecules_graph_regression as train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "#     AUTORELOAD IPYTHON EXTENSION FOR RELOADING IMPORTED MODULES\n",
    "# \"\"\"\n",
    "\n",
    "def in_ipynb():\n",
    "    try:\n",
    "        cfg = get_ipython().config \n",
    "        return True\n",
    "    except NameError:\n",
    "        return False\n",
    "    \n",
    "notebook_mode = in_ipynb()\n",
    "print(notebook_mode)\n",
    "\n",
    "if notebook_mode == True:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    GPU Setup\n",
    "\"\"\"\n",
    "def gpu_setup(use_gpu, gpu_id):\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n",
    "\n",
    "    if torch.cuda.is_available() and use_gpu:\n",
    "        print('cuda available with GPU:',torch.cuda.get_device_name(0))\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        print('cuda not available')\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy of params for testing purposes -> change to read from txt file from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "     Loading variables from config file\n",
    "\"\"\"\n",
    "def loadConfigFile(config_file):\n",
    "    # Read data from the YAML file\n",
    "    with open(config_file, 'r') as file:\n",
    "        loaded_data = yaml.safe_load(file)\n",
    "\n",
    "    # Extract data from the loaded dictionary\n",
    "    DATASET_NAME = loaded_data['Dataset']\n",
    "    MODEL_NAME = loaded_data['Model']\n",
    "    net_params = loaded_data['net_params']\n",
    "    params = loaded_data['params']\n",
    "\n",
    "    # Add device back \n",
    "    net_params['device'] = gpu_setup(net_params['gpu_id'] != -1, net_params['gpu_id'])\n",
    "\n",
    "    return DATASET_NAME, MODEL_NAME, net_params, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Load data set\n",
    "\"\"\"\n",
    "def LoadAndSplitDataset(DATASET_NAME,net_params,params):\n",
    "    dataset = LoadData(DATASET_NAME)\n",
    "\n",
    "    if MODEL_NAME in ['GCN', 'GAT']:\n",
    "            if net_params['self_loop']:\n",
    "                print(\"[!] Adding graph self-loops for GCN/GAT models (central node trick).\")\n",
    "                dataset._add_self_loops()\n",
    "                \n",
    "    if MODEL_NAME in ['GatedGCN']:\n",
    "        if net_params['pos_enc']:\n",
    "            print(\"[!] Adding graph positional encoding.\")\n",
    "            dataset._add_positional_encodings(net_params['pos_enc_dim'])\n",
    "            print('Time PE:',time.time()-t0)\n",
    "\n",
    "    trainset, valset, testset = dataset.train, dataset.val, dataset.test\n",
    "    # batching exception for Diffpool\n",
    "    drop_last = True if MODEL_NAME == 'DiffPool' else False\n",
    "\n",
    "    if MODEL_NAME in ['RingGNN', '3WLGNN']:\n",
    "            # import train functions specific for WLGNNs\n",
    "            from train.train_molecules_graph_regression import train_epoch_dense as train_epoch, evaluate_network_dense as evaluate_network\n",
    "            from functools import partial # util function to pass edge_feat to collate function\n",
    "\n",
    "            train_loader = DataLoader(trainset, shuffle=True, collate_fn=partial(dataset.collate_dense_gnn, edge_feat=net_params['edge_feat']))\n",
    "            val_loader = DataLoader(valset, shuffle=False, collate_fn=partial(dataset.collate_dense_gnn, edge_feat=net_params['edge_feat']))\n",
    "            test_loader = DataLoader(testset, shuffle=False, collate_fn=partial(dataset.collate_dense_gnn, edge_feat=net_params['edge_feat']))\n",
    "            \n",
    "    else:\n",
    "        # import train functions for all other GNNs\n",
    "        from train.train_molecules_graph_regression import train_epoch_sparse as train_epoch, evaluate_network_sparse as evaluate_network\n",
    "        \n",
    "        train_loader = DataLoader(trainset, batch_size=params['batch_size'], shuffle=True, drop_last=drop_last, collate_fn=dataset.collate)\n",
    "        val_loader = DataLoader(valset, batch_size=params['batch_size'], shuffle=False, drop_last=drop_last, collate_fn=dataset.collate)\n",
    "        test_loader = DataLoader(testset, batch_size=params['batch_size'], shuffle=False, drop_last=drop_last, collate_fn=dataset.collate)\n",
    "\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    IMPORTING MODELS\n",
    "\"\"\"\n",
    "def evalModel(model_file,MODEL_NAME, net_params, train_loader, val_loader, test_loader):\n",
    "    model = gnn_model(MODEL_NAME, net_params)\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()\n",
    "\n",
    "    _, test_mae = train.evaluate_network_sparse(model, net_params[\"device\"], test_loader, 0)\n",
    "    print(\"Train MAE: {:.4f}\".format(test_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda not available\n",
      "[I] Loading dataset ZINC...\n",
      "train, test, val sizes : 10000 1000 1000\n",
      "[I] Finished loading.\n",
      "[I] Data load time: 18.0037s\n",
      "Train MAE: 0.6608\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    modelPath = \"out/molecules_graph_regression/GCN_Test_0\"\n",
    "    DATASET_NAME, MODEL_NAME, net_params, params = loadConfigFile(modelPath + \"/config.yaml\")\n",
    "    train_loader, val_loader, test_loader = LoadAndSplitDataset(DATASET_NAME,net_params,params)\n",
    "    \n",
    "    evalModel(modelPath + \"/final.pkl\", MODEL_NAME, net_params, train_loader, val_loader, test_loader)\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
